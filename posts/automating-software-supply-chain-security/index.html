<!doctype html><html lang=en dir=auto data-theme=dark><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Software supply chain data fatigue and what I’ve learned from SBOM, vulnerability reports | Mark Chmarny</title><meta name=keywords content="s3c,sbom,cve,metric,data"><meta name=description content="If you are doing any vulnerability detection in your software release pipeline today, you are already familiar with the volumes of data these scanners can generate. That dataset gets significantly larger when you add things like license scanning and Software Bill of Materials (SBOM) generation. That volume of data gets further compounded with each highly-automated pipeline you operate. This can quickly lead to what I refer to as a Software Supply Chain Security (S3C) data fatigue, as many vulnerabilities you’ll discover you simply can’t do anything about. There is an actionable signal in there actually, it’s just hard to find it in the midst of all the noise."><meta name=author content="Mark Chmarny"><link rel=canonical href=https://blog.chmarny.com/posts/automating-software-supply-chain-security/><link crossorigin=anonymous href=/assets/css/stylesheet.5bef4b698d33ee9d58f22269d129fe3c725b1b3356a4ed1ded10651b773f88c5.css integrity="sha256-W+9LaY0z7p1Y8iJp0Sn+PHJbGzNWpO0d7RBlG3c/iMU=" rel="preload stylesheet" as=style><link rel=icon href=https://blog.chmarny.com/favicons/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://blog.chmarny.com/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://blog.chmarny.com/favicon-32x32.png><link rel=apple-touch-icon href=https://blog.chmarny.com/apple-touch-icon.png><link rel=mask-icon href=https://blog.chmarny.com/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://blog.chmarny.com/posts/automating-software-supply-chain-security/><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><script>localStorage.getItem("pref-theme")==="light"&&(document.querySelector("html").dataset.theme="light")</script><meta property="og:url" content="https://blog.chmarny.com/posts/automating-software-supply-chain-security/"><meta property="og:site_name" content="Mark Chmarny"><meta property="og:title" content="Software supply chain data fatigue and what I’ve learned from SBOM, vulnerability reports"><meta property="og:description" content="If you are doing any vulnerability detection in your software release pipeline today, you are already familiar with the volumes of data these scanners can generate. That dataset gets significantly larger when you add things like license scanning and Software Bill of Materials (SBOM) generation. That volume of data gets further compounded with each highly-automated pipeline you operate. This can quickly lead to what I refer to as a Software Supply Chain Security (S3C) data fatigue, as many vulnerabilities you’ll discover you simply can’t do anything about. There is an actionable signal in there actually, it’s just hard to find it in the midst of all the noise."><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2023-01-11T07:09:34-08:00"><meta property="article:modified_time" content="2023-01-11T07:09:34-08:00"><meta property="article:tag" content="S3c"><meta property="article:tag" content="Sbom"><meta property="article:tag" content="Cve"><meta property="article:tag" content="Metric"><meta property="article:tag" content="Data"><meta property="og:image" content="https://blog.chmarny.com/images/site-feature-image.png"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://blog.chmarny.com/images/site-feature-image.png"><meta name=twitter:title content="Software supply chain data fatigue and what I’ve learned from SBOM, vulnerability reports"><meta name=twitter:description content="If you are doing any vulnerability detection in your software release pipeline today, you are already familiar with the volumes of data these scanners can generate. That dataset gets significantly larger when you add things like license scanning and Software Bill of Materials (SBOM) generation. That volume of data gets further compounded with each highly-automated pipeline you operate. This can quickly lead to what I refer to as a Software Supply Chain Security (S3C) data fatigue, as many vulnerabilities you’ll discover you simply can’t do anything about. There is an actionable signal in there actually, it’s just hard to find it in the midst of all the noise."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://blog.chmarny.com/posts/"},{"@type":"ListItem","position":2,"name":"Software supply chain data fatigue and what I’ve learned from SBOM, vulnerability reports","item":"https://blog.chmarny.com/posts/automating-software-supply-chain-security/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Software supply chain data fatigue and what I’ve learned from SBOM, vulnerability reports","name":"Software supply chain data fatigue and what I’ve learned from SBOM, vulnerability reports","description":"If you are doing any vulnerability detection in your software release pipeline today, you are already familiar with the volumes of data these scanners can generate. That dataset gets significantly larger when you add things like license scanning and Software Bill of Materials (SBOM) generation. That volume of data gets further compounded with each highly-automated pipeline you operate. This can quickly lead to what I refer to as a Software Supply Chain Security (S3C) data fatigue, as many vulnerabilities you’ll discover you simply can’t do anything about. There is an actionable signal in there actually, it’s just hard to find it in the midst of all the noise.\n","keywords":["s3c","sbom","cve","metric","data"],"articleBody":"If you are doing any vulnerability detection in your software release pipeline today, you are already familiar with the volumes of data these scanners can generate. That dataset gets significantly larger when you add things like license scanning and Software Bill of Materials (SBOM) generation. That volume of data gets further compounded with each highly-automated pipeline you operate. This can quickly lead to what I refer to as a Software Supply Chain Security (S3C) data fatigue, as many vulnerabilities you’ll discover you simply can’t do anything about. There is an actionable signal in there actually, it’s just hard to find it in the midst of all the noise.\nOver the last year, there has been a growing number of existing and new security focused ISVs who are starting to now provide integrated products to help you management of all this data and offer algorithms to automate the discovery (e.g. Chainguard, Mend, Synopsys, Sonatype, Snyk). In GCP, we’ve also added Container Analysis API to enable metadata management.\nTo learn more about S3C data, and to better understand the challenges in this space, I’ve put together a simple solution called disco to:\nContinuously discover container images used in my workloads across multiple GCP projects, regions, and runtimes Easily plug different open source image scanners (e.g. syft and trivy for SBOM, grype, osv-scanner, snyk, and trivy for vulnerabilities, and some combination of each of these for licenses) Automatically manage data exports for: Raw scanner reports into GCS bucket Key data feature metrics into Cloud Monitoring time-series Synthesized data into BigQuery tables Here is a high-level topology of the disco solution:\nThe four challenges I’ve been thinking about in relation to the above-described S3C data noise, along with a simple pragmatic approaches I wanted to evaluate are:\nLarge number of data sources (i.e. many images with many versions in container registry, which ones are actually used?) - Scope sources down to only what’s actually being used at any given time to underpin the live services. Ideally the discovery shifts left and integrates into release pipeline, but, if the release frequency is not high (order of days, or more), this may be valuable addition, as new vulnerabilities are discovered all the time (i.e. Day 0). Multiple data formats (i.e. SPDX, CycloneDX, which version?) - Normalize the data into a consistent set that maps the key identities (e.g. packages, files, relationships, licenses, vulnerabilities, etc.), across multiple formats. Hard to parse signal in raw data (hard to reason over values in JSON or YAML file) - Identify key features and automate metering to create a change detection system to focus on significant events using threshold rules scoped to a project, deployment, runtime, image, or even single package. Point-in-time perspective (hard to compare multiple sources or capture deviation over time) - Enable forensic (historical) analysis over data spanning longer periods of time to detect trends and potential gaps using commonly known skills and technologies (e.g. SQL) to enable a broader number of reporting tools and audiences. Here is how I’ve implemented each one of these approaches and what I have learned in the process:\nImage Discovery To find images that are being actively used in live services I’ve written a simple Go client. That client queries the GCP API for projects, and traverses the active deployments in each runtime to find the specific image digest. This is basically equivalent to this gcloud commands like this:\ncurl -H \"Content-Type: application/json; charset=utf-8\" \\ -H \"Authorization: Bearer $(gcloud auth application-default print-access-token)\" \\ \"https://cloudresourcemanager.googleapis.com/v1/projects\" The project, regions, runtimes, and service that client discovers are automatically scoped to the identity under which the disco service operates. This makes it easy to manage cross-project discovery using IAM roles.\nNew services are deployed all the time, so the disco service uses a cron job in Cloud Scheduler to continuously execute the discovery service (defaults to hourly). Technically, each new deployment in these runtimes has a corresponding event in Eventarc, so ad-hoc scans for newly updated services are possible. Still, to keep things simple, disco operates in batches.\nThe actual digest that underpins the live service in Cloud Run is stored on the revision, and Cloud Run can split traffic across multiple revisions. To deal with this, disco uses API filters and traverses the revisions to identify all of the “active” ones. Also, while somewhat counter-intuitive, Cloud Functions does actually build images behind the scene when you deploy a new function. These are managed by GCP, so the GCF API doesn’t expose the actual digest, but it does provide the revision ID, which can be used against the Cloud Run API to discover the actual digest.\nData Normalization To extract data from each one of the discovered images, disco uses one of the preconfigured open source scanners. These scanners generate reports which disco then uses to extract three main entities:\nPackages SBOM file generated by scanner (default: SPDX v2.2 using trivy although I have experimented with the other formats I found in the set the Chainguard team maintains in their (s)bom-shelter). The disco mapping is mostly flattened ‘.packages’ data with file level metadata. I’ve started experimenting with traversing the ‘.relationships’ graph but this turned out to be a lot more complicated than I expected, and didn’t really add that much value to the simple use-case I was after. (I’d like to integrate GUAC here in the future to normalizing entity identities via mapping standard relationships using graph DB)\nVulnerabilities From a vulnerability report generated by one of the OSS scanners (default: Trivy) for all layers in each image (SPDX v3.0 will include vulnerability data, so this will remove the need for additional scans and different parsers. For now though, the subset of packages with vulnerabilities need to be mapped to all of the packages found in the image. I’ve also started to look into the CVSS scores from different vendors, but still have much to learn about the lifecycle and implications of these so for now just using the “simple” severity\nLicenses From OS and package license report used in each image using an OSS scanner (default: trivy). Technically, you could pull the license information from SBOM, but the dataset in the explicit report is more complete and I was able to avoid the whole “declared” (by vendor) vs “concluded” dance. Trivy also provides a “confidence” indicator which helps in downstream analysis\nOnce extracted, in addition to all the other data, disco also normalizes the shape of the key elements for all of these identities to enable cross-reference:\nBatchID - single ID for the entire batch to make deduplication easier ImageURI - image URI (without tag or sha) for queries spanning multiple versions ImageSHA - image sha to ensure unique identity for specific image version Package - fully qualified package name PackageVersion - canonical version of the package Updated - timestamp when the data element was extracted Metrics Export\nEven with only a small number of services across a couple of runtimes, disco will quickly generate thousands of data points. To ensure that the meaningful signal is detected early, disco exports custom metrics from each scan, for example:\ndisco/vulnerability/severity - vulnerability count for each severity (labels: severity value, project, service, runtime, image) disco/license/image - licenses per image (labels: license kind, project, service, runtime) disco/package/image - packages per image (labels: project, service, runtime) Once exported, I was able to use Metric Explorer to review the generated metrics:\nI was also able to create notifications policies based on metric thresholds to alert using one of the supported channels (e.g. Mobile device app notification, Slack, Email, SMS, etc), and route the events to another process downstream using Pub/Sub (or Webhook).\nAt the end, only the severity metric turned out to be of significant value here. Packages, and licenses have way too high cardinality for individual metrics, and generic point metrics are fun for graphs, but I’m not sure anyone would actuate something on that (even when generalized, because there are at least ten diff ways to spell Artistic-2).\nForensic Analysis Each of the data sources in disco (image, package, license, and vulnerability), has many data points, so for broader analysis over a longer period of time, disco also batch exports its data to BigQuery tables. The schema of each one of the tables is available here. To ensure the queries are performant over time regardless of volumes, disco partitions each table per day. More on partitioned tables here.\nOnce in BigQuery, I was able to use standard SQL to query over that data, using different joins across all the three main identities (license, package, vulnerability). I haven’t had the time to do it yet, but it would be interesting to combine the reports across different CVE sources like NVD or VULDB, to see how each reports CVEs over time. Using this setup, I was however able to identify a few interesting deviations over time. For example:\nPackage version changes between versions, and cross-service distribution Vulnerability (CVE) severity and score changes Package license changes (yep, those do happen), see FOSSA And because BigQuery has a rich ecosystem of visualization tools, I can also easily create reports from all this collected data like this one in Looker Studio which actually allows for drill-downs (select image and only its vulnerabilities, licenses, and package distribution).\nIn Summary If nothing else, building disco was an educational experience. I gained appreciation for some of the idiosyncrasies in SBOM formats, I’ve learned about the different vulnerability data sources, and evaluated the tradeoffs in data aggregation and synthesis. This already happens in companies with more advanced SecOps, I do think, eventually, this data will more broadly drive policy decisions, be instrumental in audits, and used to manage risk. This data could also potentially drive better development practices. All in all, this is an exciting space with much, much more room for innovation. I look forward to digging deeper.\nIf you are interested, the disco source code, and Terraform-based deployment with prebuilt images, is available in my repo in github.com/mchmarny/disco. There also is a CLI you can use locally (installation instructions here). Hope you found this helpful.\n","wordCount":"1679","inLanguage":"en","image":"https://blog.chmarny.com/images/site-feature-image.png","datePublished":"2023-01-11T07:09:34-08:00","dateModified":"2023-01-11T07:09:34-08:00","author":{"@type":"Person","name":"Mark Chmarny"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://blog.chmarny.com/posts/automating-software-supply-chain-security/"},"publisher":{"@type":"Organization","name":"Mark Chmarny","logo":{"@type":"ImageObject","url":"https://blog.chmarny.com/favicons/favicon.ico"}}}</script></head><body id=top><header class=header><nav class=nav><div class=logo><a href=https://blog.chmarny.com/ accesskey=h title="Mark Chmarny (Alt + H)">Mark Chmarny</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://blog.chmarny.com/posts/ title=Posts><span>Posts</span></a></li><li><a href=https://blog.chmarny.com/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li><li><a href=https://blog.chmarny.com/tags/ title=Tags><span>Tags</span></a></li><li><a href=https://blog.chmarny.com/archives/ title=Archives><span>Archives</span></a></li><li><a href=https://blog.chmarny.com/about/ title=About><span>About</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://blog.chmarny.com/>Home</a>&nbsp;»&nbsp;<a href=https://blog.chmarny.com/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">Software supply chain data fatigue and what I’ve learned from SBOM, vulnerability reports</h1><div class=post-meta><span title='2023-01-11 07:09:34 -0800 -0800'>2023-01-11</span>&nbsp;·&nbsp;<span>8 min</span>&nbsp;·&nbsp;<span>Mark Chmarny</span></div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#image-discovery aria-label="Image Discovery">Image Discovery</a></li><li><a href=#data-normalization aria-label="Data Normalization">Data Normalization</a><ul><ul><li><a href=#packages aria-label=Packages>Packages</a></li><li><a href=#vulnerabilities aria-label=Vulnerabilities>Vulnerabilities</a></li><li><a href=#licenses aria-label=Licenses>Licenses</a></li></ul></ul></li><li><a href=#forensic-analysis aria-label="Forensic Analysis">Forensic Analysis</a></li><li><a href=#in-summary aria-label="In Summary">In Summary</a></li></ul></div></details></div><div class=post-content><p>If you are doing any vulnerability detection in your software release pipeline today, you are already familiar with the volumes of data these scanners can generate. That dataset gets significantly larger when you add things like license scanning and <a href=https://www.cisa.gov/sbom>Software Bill of Materials</a> (SBOM) generation. That volume of data gets further compounded with each highly-automated pipeline you operate. This can quickly lead to what I refer to as a Software Supply Chain Security (S3C) data fatigue, as many vulnerabilities you’ll discover you simply can’t do anything about. There is an actionable signal in there actually, it’s just hard to find it in the midst of all the noise.</p><p>Over the last year, there has been a growing number of existing and new security focused ISVs who are starting to now provide integrated products to help you management of all this data and offer algorithms to automate the discovery (e.g. <a href=https://www.chainguard.dev/chainguard-enforce>Chainguard</a>, <a href=https://www.mend.io/>Mend</a>, <a href=https://www.synopsys.com/software-integrity.html>Synopsys</a>, <a href=https://www.sonatype.com/solutions/appsec-professionals>Sonatype</a>, <a href=https://snyk.io/>Snyk</a>). In GCP, we’ve also added <a href=https://cloud.google.com/container-analysis/docs/container-analysis>Container Analysis</a> API to enable metadata management.</p><p>To learn more about S3C data, and to better understand the challenges in this space, I’ve put together a simple solution called <a href=https://github.com/mchmarny/disco>disco</a> to:</p><ul><li>Continuously discover container images used in my workloads across multiple <a href=https://cloud.google.com/>GCP</a> projects, regions, and runtimes</li><li>Easily plug different open source image scanners (e.g. <a href=https://github.com/anchore/syft>syft</a> and <a href=https://github.com/aquasecurity/trivy>trivy</a> for SBOM, <a href=https://github.com/anchore/grype>grype</a>, <a href=https://github.com/google/osv-scanner>osv-scanner</a>, <a href=https://github.com/snyk/cli>snyk</a>, and <a href=https://github.com/aquasecurity/trivy>trivy</a> for vulnerabilities, and some combination of each of these for licenses)</li><li>Automatically manage data exports for:<ul><li>Raw scanner reports into <a href=https://cloud.google.com/storage>GCS</a> bucket</li><li>Key data feature metrics into <a href=https://cloud.google.com/monitoring>Cloud Monitoring</a> time-series</li><li>Synthesized data into <a href=https://cloud.google.com/bigquery>BigQuery</a> tables</li></ul></li></ul><p>Here is a high-level topology of the disco solution:</p><p><img alt="disco topology" loading=lazy src=/images/s3c-diagram.png></p><p>The four challenges I’ve been thinking about in relation to the above-described S3C data noise, along with a simple pragmatic approaches I wanted to evaluate are:</p><ul><li><strong>Large number of data sources</strong> (i.e. many images with many versions in container registry, which ones are actually used?) - Scope sources down to only what’s actually being used at any given time to underpin the live services. Ideally the discovery shifts left and integrates into release pipeline, but, if the release frequency is not high (order of days, or more), this may be valuable addition, as new vulnerabilities are discovered all the time (i.e. Day 0).</li><li><strong>Multiple data formats</strong> (i.e. SPDX, CycloneDX, which version?) - Normalize the data into a consistent set that maps the key identities (e.g. packages, files, relationships, licenses, vulnerabilities, etc.), across multiple formats.</li><li><strong>Hard to parse signal in raw data</strong> (hard to reason over values in JSON or YAML file) - Identify key features and automate metering to create a change detection system to focus on significant events using threshold rules scoped to a project, deployment, runtime, image, or even single package.</li><li><strong>Point-in-time perspective</strong> (hard to compare multiple sources or capture deviation over time) - Enable forensic (historical) analysis over data spanning longer periods of time to detect trends and potential gaps using commonly known skills and technologies (e.g. SQL) to enable a broader number of reporting tools and audiences.</li></ul><p>Here is how I’ve implemented each one of these approaches and what I have learned in the process:</p><h1 id=image-discovery>Image Discovery<a hidden class=anchor aria-hidden=true href=#image-discovery>#</a></h1><p>To find images that are being actively used in live services I’ve written a simple Go client. That client queries the GCP API for projects, and traverses the active deployments in each runtime to find the specific image digest. This is basically equivalent to this <a href=https://cloud.google.com/sdk/gcloud>gcloud</a> commands like this:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-shell data-lang=shell><span class=line><span class=cl>curl -H <span class=s2>&#34;Content-Type: application/json; charset=utf-8&#34;</span> <span class=se>\
</span></span></span><span class=line><span class=cl>     -H <span class=s2>&#34;Authorization: Bearer </span><span class=k>$(</span>gcloud auth application-default print-access-token<span class=k>)</span><span class=s2>&#34;</span> <span class=se>\
</span></span></span><span class=line><span class=cl>     <span class=s2>&#34;https://cloudresourcemanager.googleapis.com/v1/projects&#34;</span>
</span></span></code></pre></div><p>The project, regions, runtimes, and service that client discovers are automatically scoped to the identity under which the disco service operates. This makes it easy to manage cross-project discovery using <a href=https://cloud.google.com/iam/docs/understanding-roles>IAM roles</a>.</p><p>New services are deployed all the time, so the disco service uses a cron job in Cloud Scheduler to continuously execute the discovery service (defaults to hourly). Technically, each new deployment in these runtimes has a corresponding event in <a href=https://cloud.google.com/functions/docs/calling/eventarc>Eventarc</a>, so ad-hoc scans for newly updated services are possible. Still, to keep things simple, disco operates in batches.</p><p>The actual digest that underpins the live service in Cloud Run is stored on the revision, and Cloud Run can split traffic across multiple revisions. To deal with this, disco uses API filters and traverses the revisions to identify all of the “active” ones. Also, while somewhat counter-intuitive, Cloud Functions does actually <a href=https://cloud.google.com/functions/docs/building>build images behind the scene</a> when you deploy a new function. These are managed by GCP, so the GCF API doesn’t expose the actual digest, but it does provide the revision ID, which can be used against the Cloud Run API to discover the actual digest.</p><h1 id=data-normalization>Data Normalization<a hidden class=anchor aria-hidden=true href=#data-normalization>#</a></h1><p>To extract data from each one of the discovered images, disco uses one of the preconfigured open source scanners. These scanners generate reports which disco then uses to extract three main entities:</p><h3 id=packages>Packages<a hidden class=anchor aria-hidden=true href=#packages>#</a></h3><p>SBOM file generated by scanner (default: <a href=https://spdx.dev/wp-content/uploads/sites/41/2020/08/SPDX-specification-2-2.pdf>SPDX v2.2</a> using trivy although I have experimented with the other formats I found in the set the Chainguard team maintains in their <a href=https://github.com/chainguard-dev/bom-shelter>(s)bom-shelter</a>). The disco mapping is mostly flattened ‘.packages’ data with file level metadata. I’ve started experimenting with traversing the ‘.relationships’ graph but this turned out to be a lot more complicated than I expected, and didn’t really add that much value to the simple use-case I was after. (I’d like to integrate <a href=https://github.com/guacsec/guac>GUAC</a> here in the future to normalizing entity identities via mapping standard relationships using graph DB)</p><h3 id=vulnerabilities>Vulnerabilities<a hidden class=anchor aria-hidden=true href=#vulnerabilities>#</a></h3><p>From a vulnerability report generated by one of the OSS scanners (default: Trivy) for all layers in each image (<a href=https://github.com/spdx/spdx-spec/milestone/3>SPDX v3.0</a> will include vulnerability data, so this will remove the need for additional scans and different parsers. For now though, the subset of packages with vulnerabilities need to be mapped to all of the packages found in the image. I’ve also started to look into the CVSS scores from different vendors, but still have much to learn about the lifecycle and implications of these so for now just using the “simple” severity</p><h3 id=licenses>Licenses<a hidden class=anchor aria-hidden=true href=#licenses>#</a></h3><p>From OS and package license report used in each image using an OSS scanner (default: trivy). Technically, you could pull the license information from SBOM, but the dataset in the explicit report is more complete and I was able to avoid the whole “declared” (by vendor) vs “concluded” dance. Trivy also provides a “confidence” indicator which helps in downstream analysis</p><p>Once extracted, in addition to all the other data, disco also normalizes the shape of the key elements for all of these identities to enable cross-reference:</p><ul><li><strong>BatchID</strong> - single ID for the entire batch to make deduplication easier</li><li><strong>ImageURI</strong> - image URI (without tag or sha) for queries spanning multiple versions</li><li><strong>ImageSHA</strong> - image sha to ensure unique identity for specific image version</li><li><strong>Package</strong> - fully qualified package name</li><li><strong>PackageVersion</strong> - canonical version of the package</li><li><strong>Updated</strong> - timestamp when the data element was extracted</li></ul><p><strong>Metrics Export</strong></p><p>Even with only a small number of services across a couple of runtimes, disco will quickly generate thousands of data points. To ensure that the meaningful signal is detected early, disco exports custom metrics from each scan, for example:</p><ul><li><strong>disco/vulnerability/severity</strong> - vulnerability count for each severity (labels: severity value, project, service, runtime, image)</li><li><strong>disco/license/image</strong> - licenses per image (labels: license kind, project, service, runtime)</li><li><strong>disco/package/image</strong> - packages per image (labels: project, service, runtime)</li></ul><p>Once exported, I was able to use <a href=https://cloud.google.com/monitoring/charts/metrics-explorer>Metric Explorer</a> to review the generated metrics:</p><p><img alt="Metric Explore" loading=lazy src=/images/s3c-metric-explore.png></p><p>I was also able to create notifications policies based on metric thresholds to alert using one of the supported channels (e.g. Mobile device app notification, Slack, Email, SMS, etc), and route the events to another process downstream using Pub/Sub (or Webhook).</p><p>At the end, only the severity metric turned out to be of significant value here. Packages, and licenses have way too high cardinality for individual metrics, and generic point metrics are fun for graphs, but I’m not sure anyone would actuate something on that (even when generalized, because there are at least ten diff ways to spell Artistic-2).</p><p><img alt="Metric Charts" loading=lazy src=/images/s3c-metric-charts.png></p><h1 id=forensic-analysis>Forensic Analysis<a hidden class=anchor aria-hidden=true href=#forensic-analysis>#</a></h1><p>Each of the data sources in disco (image, package, license, and vulnerability), has many data points, so for broader analysis over a longer period of time, disco also batch exports its data to BigQuery tables. The schema of each one of the tables is available here. To ensure the queries are performant over time regardless of volumes, disco partitions each table per day. More on partitioned tables <a href=https://cloud.google.com/bigquery/docs/partitioned-tables>here</a>.</p><p><img alt="SQL Query" loading=lazy src=/images/s3c-sql-query.png></p><p>Once in BigQuery, I was able to use standard SQL to query over that data, using different joins across all the three main identities (license, package, vulnerability). I haven’t had the time to do it yet, but it would be interesting to combine the reports across different CVE sources like <a href=https://nvd.nist.gov/>NVD</a> or <a href=https://vuldb.com/>VULDB</a>, to see how each reports CVEs over time. Using this setup, I was however able to identify a few interesting deviations over time. For example:</p><ul><li>Package version changes between versions, and cross-service distribution</li><li>Vulnerability (CVE) severity and score changes</li><li>Package license changes (yep, those do happen), see <a href=https://docs.fossa.com/docs/generating-reports>FOSSA</a></li></ul><p>And because BigQuery has a rich ecosystem of visualization tools, I can also easily create reports from all this collected data like this one in Looker Studio which actually allows for drill-downs (select image and only its vulnerabilities, licenses, and package distribution).</p><p><img alt="Vuln Dashbaord" loading=lazy src=/images/s3c-dashboard.png></p><h1 id=in-summary>In Summary<a hidden class=anchor aria-hidden=true href=#in-summary>#</a></h1><p>If nothing else, building disco was an educational experience. I gained appreciation for some of the idiosyncrasies in SBOM formats, I’ve learned about the different vulnerability data sources, and evaluated the tradeoffs in data aggregation and synthesis. This already happens in companies with more advanced SecOps, I do think, eventually, this data will more broadly drive policy decisions, be instrumental in audits, and used to manage risk. This data could also potentially drive better development practices. All in all, this is an exciting space with much, much more room for innovation. I look forward to digging deeper.</p><p>If you are interested, the disco source code, and Terraform-based deployment with prebuilt images, is available in my repo in <a href=https://github.com/mchmarny/disco>github.com/mchmarny/disco</a>. There also is a CLI you can use locally (installation instructions <a href=https://github.com/mchmarny/disco/blob/main/docs/CLI.md>here</a>). Hope you found this helpful.</p></div><footer class=post-footer><ul class=post-tags><li><a href=https://blog.chmarny.com/tags/s3c/>S3c</a></li><li><a href=https://blog.chmarny.com/tags/sbom/>Sbom</a></li><li><a href=https://blog.chmarny.com/tags/cve/>Cve</a></li><li><a href=https://blog.chmarny.com/tags/metric/>Metric</a></li><li><a href=https://blog.chmarny.com/tags/data/>Data</a></li></ul><nav class=paginav><a class=prev href=https://blog.chmarny.com/posts/provenance-transparency-context-3-aspects-of-s3c-you-can-implement-today/><span class=title>« Prev</span><br><span>Provenance, transparency, and context, the three aspects of software supply chain security you can implement today</span>
</a><a class=next href=https://blog.chmarny.com/posts/twitter-follower-activity-monitoring-using-tweethingz/><span class=title>Next »</span><br><span>Twitter follower status monitoring made easy using TweeThingz</span></a></nav></footer></article></main><footer class=footer><span>&copy; 2026 <a href=https://blog.chmarny.com/>Mark Chmarny</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");if(menu){const e=localStorage.getItem("menu-scroll-position");e&&(menu.scrollLeft=parseInt(e,10)),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}}document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{const e=document.querySelector("html");e.dataset.theme==="dark"?(e.dataset.theme="light",localStorage.setItem("pref-theme","light")):(e.dataset.theme="dark",localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>