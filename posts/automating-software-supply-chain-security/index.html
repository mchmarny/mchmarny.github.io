<!doctype html><html dir=ltr lang=en data-theme=dark><head><title>Mark Chmarny
|
Software supply chain data fatigue and what I’ve learned from SBOM, vulnerability reports</title><meta charset=utf-8><meta name=generator content="Hugo 0.109.0"><meta name=viewport content="width=device-width,initial-scale=1,viewport-fit=cover"><meta name=description content="
      few longer thoughts, <br/>because every once in a while <br/>140 characters is just not enough


    "><link rel=stylesheet href=/css/main.min.179cc2d1e93212b8ec31560509068248e4269f4b648220d1154adb0d5e94aa7d.css integrity="sha256-F5zC0ekyErjsMVYFCQaCSOQmn0tkgiDRFUrbDV6Uqn0=" crossorigin=anonymous type=text/css><link rel=stylesheet href=/css/markupHighlight.min.058b31f17db60602cc415fd63b0427e7932fbf35c70d8e341a4c39385f5f6f3e.css integrity="sha256-BYsx8X22BgLMQV/WOwQn55MvvzXHDY40Gkw5OF9fbz4=" crossorigin=anonymous type=text/css><link rel=stylesheet href=/css/custom.min.c932a7c5e2d9eb0228f2bd2992ea2d5acb4f2715f7d229cebabea22225c38356.css integrity="sha256-yTKnxeLZ6wIo8r0pkuotWstPJxX30inOur6iIiXDg1Y=" crossorigin=anonymous media=screen><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css integrity="sha512-+4zCK9k+qNFUR5X+cKL9EIR+ZOhtIloNl9GIKS57V1MyNsYpYcUrUeQc9vNfzsWfV28IaLL3i96P9sdNyeRssA==" crossorigin=anonymous><link rel=preconnect href=https://fonts.googleapis.com><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link href="https://fonts.googleapis.com/css2?family=Indie+Flower&family=Roboto:ital,wght@0,100;0,400;0,700;1,400&display=swap" rel=stylesheet><link rel="shortcut icon" href=/favicons/favicon.ico type=image/x-icon><link rel=apple-touch-icon sizes=180x180 href=/favicons/apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=/favicons/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/favicons/favicon-16x16.png><link rel=canonical href=https://blog.chmarny.com/posts/automating-software-supply-chain-security/><script type=text/javascript src=/js/anatole-header.min.d0408165d31a17f17bba83038bf54e86121f85021bdf936382e636f0f77a952f.js integrity="sha256-0ECBZdMaF/F7uoMDi/VOhhIfhQIb35NjguY28Pd6lS8=" crossorigin=anonymous></script>
<script src=https://cdn.jsdelivr.net/npm/algoliasearch@4.5.1/dist/algoliasearch-lite.umd.js integrity="sha256-EXPXz4W6pQgfYY3yTpnDa3OH8/EPn16ciVsPQ/ypsjk=" crossorigin=anonymous></script>
<script src=https://cdn.jsdelivr.net/npm/instantsearch.js@4.8.3/dist/instantsearch.production.min.js integrity="sha256-LAGhRRdtVoD6RLo2qDQsU2mp+XVSciKRC8XPOBWmofM=" crossorigin=anonymous></script>
<script type=text/javascript src=/js/anatole-theme-switcher.min.ea8ebe268922ef9849261a1312cd65b640595e65251ce4c00534a176afd1ac0c.js integrity="sha256-6o6+Joki75hJJhoTEs1ltkBZXmUlHOTABTShdq/RrAw=" crossorigin=anonymous></script>
<script type=text/javascript src=/js/search.min.b3a692306dd9db78b63ea76fac0b1b32033f19465563f7378cf5f7dc05811ce7.js integrity="sha256-s6aSMG3Z23i2PqdvrAsbMgM/GUZVY/c3jPX33AWBHOc=" crossorigin=anonymous></script><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://blog.chmarny.com/images/site-feature-image.png"><meta name=twitter:title content="Software supply chain data fatigue and what I’ve learned from SBOM, vulnerability reports"><meta name=twitter:description content="If you are doing any vulnerability detection in your software release pipeline today, you are already familiar with the volumes of data these scanners can generate. That dataset gets significantly larger when you add things like license scanning and Software Bill of Materials (SBOM) generation."><script type=application/ld+json>{"@context":"http://schema.org","@type":"BlogPosting","articleSection":"posts","name":"Software supply chain data fatigue and what I’ve learned from SBOM, vulnerability reports","headline":"Software supply chain data fatigue and what I’ve learned from SBOM, vulnerability reports","alternativeHeadline":"","description":"
      
        If you are doing any vulnerability detection in your software release pipeline today, you are already familiar with the volumes of data these scanners can generate. That dataset gets significantly larger when you add things like license scanning and Software Bill of Materials (SBOM) generation.


      


    ","inLanguage":"en","isFamilyFriendly":"true","mainEntityOfPage":{"@type":"WebPage","@id":"https:\/\/blog.chmarny.com\/posts\/automating-software-supply-chain-security\/"},"author":{"@type":"Person","name":"Mark Chmarny"},"creator":{"@type":"Person","name":"Mark Chmarny"},"accountablePerson":{"@type":"Person","name":"Mark Chmarny"},"copyrightHolder":{"@type":"Person","name":"Mark Chmarny"},"copyrightYear":"2023","dateCreated":"2023-01-11T07:09:34.00Z","datePublished":"2023-01-11T07:09:34.00Z","dateModified":"2023-01-11T07:09:34.00Z","publisher":{"@type":"Organization","name":"Mark Chmarny","url":"https://blog.chmarny.com","logo":{"@type":"ImageObject","url":"https:\/\/blog.chmarny.com\/favicons\/favicon-32x32.png","width":"32","height":"32"}},"image":["https://blog.chmarny.com/images/site-feature-image.png"],"url":"https:\/\/blog.chmarny.com\/posts\/automating-software-supply-chain-security\/","wordCount":"1679","genre":["dev"],"keywords":["s3c","sbom","cve","metric","data"]}</script></head><body><header><div class="page-top
."><a role=button class=navbar-burger data-target=navMenu aria-label=menu aria-expanded=false><span aria-hidden=true></span>
<span aria-hidden=true></span>
<span aria-hidden=true></span></a><nav><ul class=nav__list id=navMenu><div class=nav__links><li><a href=/ title>Home</a></li><li><a href=/posts/ title>Posts</a></li><li><a href=/search/ title>Search</a></li><li><a href=/about/ title>About</a></li></div><ul><li><a class=theme-switch title="Switch Theme"><i class="fas fa-adjust fa-fw" aria-hidden=true></i></a></li></ul></ul></nav></div></header><div class=wrapper><aside><div class="sidebar
."><div class=sidebar__content><div class=logo-title><div class=title><img src=/images/profile.png alt="profile picture"><h3 title><a href=/>Mark Chmarny</a></h3><div class=description><p>few longer thoughts,<br>because every once in a while<br>140 characters is just not enough</p></div></div></div><ul class=social-links><li><a href=https://twitter.com/mchmarny rel=me aria-label=Twitter title=Twitter><i class="fab fa-twitter fa-2x" aria-hidden=true></i></a></li><li><a href=https://fosstodon.org/@mchmarny rel=me aria-label=Mastodon title=Mastodon><i class="fab fa-mastodon fa-2x" aria-hidden=true></i></a></li><li><a href=https://github.com/mchmarny rel=me aria-label=GitHub title=GitHub><i class="fab fa-github fa-2x" aria-hidden=true></i></a></li><li><a href=https://speakerdeck.com/mchmarny rel=me aria-label="Speaker Deck" title="Speaker Deck"><i class="fas fa-file-powerpoint fa-2x" aria-hidden=true></i></a></li><li><a href=mailto:blog@chmarny.com rel=me aria-label=e-mail title=e-mail><i class="fas fa-envelope fa-2x" aria-hidden=true></i></a></li></ul></div><footer class="footer footer--sidebar"><div class=by_farbox><ul class=footer__list><li class=footer__item><a rel=me href=https://fosstodon.org/@mchmarny>&copy;2022</a></li></ul></div></footer></div></aside><main><div class=autopagerize_page_element><div class=content><div class="post
."><div class=post-content><img class=post-thumbnail src=/images/s3c-header.png alt="Thumbnail image"><div class=post-title><h1>Software supply chain data fatigue and what I’ve learned from SBOM, vulnerability reports</h1></div><p>If you are doing any vulnerability detection in your software release pipeline today, you are already familiar with the volumes of data these scanners can generate. That dataset gets significantly larger when you add things like license scanning and <a href=https://www.cisa.gov/sbom>Software Bill of Materials</a> (SBOM) generation. That volume of data gets further compounded with each highly-automated pipeline you operate. This can quickly lead to what I refer to as a Software Supply Chain Security (S3C) data fatigue, as many vulnerabilities you’ll discover you simply can’t do anything about. There is an actionable signal in there actually, it’s just hard to find it in the midst of all the noise.</p><p>Over the last year, there has been a growing number of existing and new security focused ISVs who are starting to now provide integrated products to help you management of all this data and offer algorithms to automate the discovery (e.g. <a href=https://www.chainguard.dev/chainguard-enforce>Chainguard</a>, <a href=https://www.mend.io/>Mend</a>, <a href=https://www.synopsys.com/software-integrity.html>Synopsys</a>, <a href=https://www.sonatype.com/solutions/appsec-professionals>Sonatype</a>, <a href=https://snyk.io/>Snyk</a>). In GCP, we’ve also added <a href=https://cloud.google.com/container-analysis/docs/container-analysis>Container Analysis</a> API to enable metadata management.</p><p>To learn more about S3C data, and to better understand the challenges in this space, I’ve put together a simple solution called <code>disco</code> to:</p><ul><li>Continuously discover container images used in my workloads across multiple <a href=https://cloud.google.com/>GCP</a> projects, regions, and runtimes</li><li>Easily plug different open source image scanners (e.g. <a href=https://github.com/anchore/syft>syft</a> and <a href=https://github.com/aquasecurity/trivy>trivy</a> for SBOM, <a href=https://github.com/anchore/grype>grype</a>, <a href=https://github.com/google/osv-scanner>osv-scanner</a>, <a href=https://github.com/snyk/cli>snyk</a>, and <a href=https://github.com/aquasecurity/trivy>trivy</a> for vulnerabilities, and some combination of each of these for licenses)</li><li>Automatically manage data exports for:<ul><li>Raw scanner reports into <a href=https://cloud.google.com/storage>GCS</a> bucket</li><li>Key data feature metrics into <a href=https://cloud.google.com/monitoring>Cloud Monitoring</a> time-series</li><li>Synthesized data into <a href=https://cloud.google.com/bigquery>BigQuery</a> tables</li></ul></li></ul><p>Here is a high-level topology of the disco solution:</p><p><img src=/images/s3c-diagram.png alt="disco topology"></p><p>The four challenges I’ve been thinking about in relation to the above-described S3C data noise, along with a simple pragmatic approaches I wanted to evaluate are:</p><ul><li><strong>Large number of data sources</strong> (i.e. many images with many versions in container registry, which ones are actually used?) - Scope sources down to only what’s actually being used at any given time to underpin the live services. Ideally the discovery shifts left and integrates into release pipeline, but, if the release frequency is not high (order of days, or more), this may be valuable addition, as new vulnerabilities are discovered all the time (i.e. Day 0).</li><li><strong>Multiple data formats</strong> (i.e. SPDX, CycloneDX, which version?) - Normalize the data into a consistent set that maps the key identities (e.g. packages, files, relationships, licenses, vulnerabilities, etc.), across multiple formats.</li><li><strong>Hard to parse signal in raw data</strong> (hard to reason over values in JSON or YAML file) - Identify key features and automate metering to create a change detection system to focus on significant events using threshold rules scoped to a project, deployment, runtime, image, or even single package.</li><li><strong>Point-in-time perspective</strong> (hard to compare multiple sources or capture deviation over time) - Enable forensic (historical) analysis over data spanning longer periods of time to detect trends and potential gaps using commonly known skills and technologies (e.g. SQL) to enable a broader number of reporting tools and audiences.</li></ul><p>Here is how I’ve implemented each one of these approaches and what I have learned in the process:</p><h1 id=image-discovery>Image Discovery</h1><p>To find images that are being actively used in live services I’ve written a simple Go client. That client queries the GCP API for projects, and traverses the active deployments in each runtime to find the specific image digest. This is basically equivalent to this <a href=https://cloud.google.com/sdk/gcloud>gcloud</a> commands like this:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-shell data-lang=shell><span class=line><span class=cl>curl -H <span class=s2>&#34;Content-Type: application/json; charset=utf-8&#34;</span> <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>     -H <span class=s2>&#34;Authorization: Bearer </span><span class=k>$(</span>gcloud auth application-default print-access-token<span class=k>)</span><span class=s2>&#34;</span> <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>     <span class=s2>&#34;https://cloudresourcemanager.googleapis.com/v1/projects&#34;</span>
</span></span></code></pre></div><p>The project, regions, runtimes, and service that client discovers are automatically scoped to the identity under which the disco service operates. This makes it easy to manage cross-project discovery using <a href=https://cloud.google.com/iam/docs/understanding-roles>IAM roles</a>.</p><p>New services are deployed all the time, so the disco service uses a cron job in Cloud Scheduler to continuously execute the discovery service (defaults to hourly). Technically, each new deployment in these runtimes has a corresponding event in <a href=https://cloud.google.com/functions/docs/calling/eventarc>Eventarc</a>, so ad-hoc scans for newly updated services are possible. Still, to keep things simple, disco operates in batches.</p><p>The actual digest that underpins the live service in Cloud Run is stored on the revision, and Cloud Run can split traffic across multiple revisions. To deal with this, disco uses API filters and traverses the revisions to identify all of the “active” ones. Also, while somewhat counter-intuitive, Cloud Functions does actually <a href=https://cloud.google.com/functions/docs/building>build images behind the scene</a> when you deploy a new function. These are managed by GCP, so the GCF API doesn’t expose the actual digest, but it does provide the revision ID, which can be used against the Cloud Run API to discover the actual digest.</p><h1 id=data-normalization>Data Normalization</h1><p>To extract data from each one of the discovered images, disco uses one of the preconfigured open source scanners. These scanners generate reports which disco then uses to extract three main entities:</p><h3 id=packages>Packages</h3><p>SBOM file generated by scanner (default: <a href=https://spdx.dev/wp-content/uploads/sites/41/2020/08/SPDX-specification-2-2.pdf>SPDX v2.2</a> using trivy although I have experimented with the other formats I found in the set the Chainguard team maintains in their <a href=https://github.com/chainguard-dev/bom-shelter>(s)bom-shelter</a>). The disco mapping is mostly flattened ‘.packages’ data with file level metadata. I’ve started experimenting with traversing the ‘.relationships’ graph but this turned out to be a lot more complicated than I expected, and didn’t really add that much value to the simple use-case I was after. (I’d like to integrate <a href=https://github.com/guacsec/guac>GUAC</a> here in the future to normalizing entity identities via mapping standard relationships using graph DB)</p><h3 id=vulnerabilities>Vulnerabilities</h3><p>From a vulnerability report generated by one of the OSS scanners (default: Trivy) for all layers in each image (<a href=https://github.com/spdx/spdx-spec/milestone/3>SPDX v3.0</a> will include vulnerability data, so this will remove the need for additional scans and different parsers. For now though, the subset of packages with vulnerabilities need to be mapped to all of the packages found in the image. I’ve also started to look into the CVSS scores from different vendors, but still have much to learn about the lifecycle and implications of these so for now just using the “simple” severity</p><h3 id=licenses>Licenses</h3><p>From OS and package license report used in each image using an OSS scanner (default: trivy). Technically, you could pull the license information from SBOM, but the dataset in the explicit report is more complete and I was able to avoid the whole “declared” (by vendor) vs “concluded” dance. Trivy also provides a “confidence” indicator which helps in downstream analysis</p><p>Once extracted, in addition to all the other data, disco also normalizes the shape of the key elements for all of these identities to enable cross-reference:</p><ul><li><strong>BatchID</strong> - single ID for the entire batch to make deduplication easier</li><li><strong>ImageURI</strong> - image URI (without tag or sha) for queries spanning multiple versions</li><li><strong>ImageSHA</strong> - image sha to ensure unique identity for specific image version</li><li><strong>Package</strong> - fully qualified package name</li><li><strong>PackageVersion</strong> - canonical version of the package</li><li><strong>Updated</strong> - timestamp when the data element was extracted</li></ul><p><strong>Metrics Export</strong></p><p>Even with only a small number of services across a couple of runtimes, disco will quickly generate thousands of data points. To ensure that the meaningful signal is detected early, disco exports custom metrics from each scan, for example:</p><ul><li><strong>disco/vulnerability/severity</strong> - vulnerability count for each severity (labels: severity value, project, service, runtime, image)</li><li><strong>disco/license/image</strong> - licenses per image (labels: license kind, project, service, runtime)</li><li><strong>disco/package/image</strong> - packages per image (labels: project, service, runtime)</li></ul><p>Once exported, I was able to use <a href=https://cloud.google.com/monitoring/charts/metrics-explorer>Metric Explorer</a> to review the generated metrics:</p><p><img src=/images/s3c-metric-explore.png alt="Metric Explore"></p><p>I was also able to create notifications policies based on metric thresholds to alert using one of the supported channels (e.g. Mobile device app notification, Slack, Email, SMS, etc), and route the events to another process downstream using Pub/Sub (or Webhook).</p><p>At the end, only the severity metric turned out to be of significant value here. Packages, and licenses have way too high cardinality for individual metrics, and generic point metrics are fun for graphs, but I’m not sure anyone would actuate something on that (even when generalized, because there are at least ten diff ways to spell Artistic-2).</p><p><img src=/images/s3c-metric-charts.png alt="Metric Charts"></p><h1 id=forensic-analysis>Forensic Analysis</h1><p>Each of the data sources in disco (image, package, license, and vulnerability), has many data points, so for broader analysis over a longer period of time, disco also batch exports its data to BigQuery tables. The schema of each one of the tables is available here. To ensure the queries are performant over time regardless of volumes, disco partitions each table per day. More on partitioned tables <a href=https://cloud.google.com/bigquery/docs/partitioned-tables>here</a>.</p><p><img src=/images/s3c-sql-query.png alt="SQL Query"></p><p>Once in BigQuery, I was able to use standard SQL to query over that data, using different joins across all the three main identities (license, package, vulnerability). I haven’t had the time to do it yet, but it would be interesting to combine the reports across different CVE sources like <a href=https://nvd.nist.gov/>NVD</a> or <a href=https://vuldb.com/>VULDB</a>, to see how each reports CVEs over time. Using this setup, I was however able to identify a few interesting deviations over time. For example:</p><ul><li>Package version changes between versions, and cross-service distribution</li><li>Vulnerability (CVE) severity and score changes</li><li>Package license changes (yep, those do happen), see <a href=https://app.fossa.com/projects>FOSSA</a></li></ul><p>And because BigQuery has a rich ecosystem of visualization tools, I can also easily create reports from all this collected data like this one in Looker Studio which actually allows for drill-downs (select image and only its vulnerabilities, licenses, and package distribution).</p><p><img src=/images/s3c-dashboard.png alt="Vuln Dashbaord"></p><h1 id=in-summary>In Summary</h1><p>If nothing else, building disco was an educational experience. I gained appreciation for some of the idiosyncrasies in SBOM formats, I’ve learned about the different vulnerability data sources, and evaluated the tradeoffs in data aggregation and synthesis. This already happens in companies with more advanced SecOps, I do think, eventually, this data will more broadly drive policy decisions, be instrumental in audits, and used to manage risk. This data could also potentially drive better development practices. All in all, this is an exciting space with much, much more room for innovation. I look forward to digging deeper.</p><p>If you are interested, the disco source code, and Terraform-based deployment with prebuilt images, is available in my repo in <a href=https://github.com/mchmarny/disco>github.com/mchmarny/disco</a>. There also is a CLI you can use locally (installation instructions <a href=https://github.com/mchmarny/disco/blob/main/docs/CLI.md>here</a>). Hope you found this helpful.</p></div><div class=post-footer><div class=info><span class=separator><a class=category href=/categories/dev/>dev</a></span>
<span class=separator><a class=tag href=/tags/s3c/>s3c</a><a class=tag href=/tags/sbom/>sbom</a><a class=tag href=/tags/cve/>cve</a><a class=tag href=/tags/metric/>metric</a><a class=tag href=/tags/data/>data</a></span></div></div></div></div></div></main></div><footer class="footer footer--base"><div class=by_farbox><ul class=footer__list><li class=footer__item><a rel=me href=https://fosstodon.org/@mchmarny>&copy;2022</a></li></ul></div></footer></body></html>